#!/usr/bin/env bash

mkdir -p /mnt/bert
mkdir -p /results
gcsfuse --stat-cache-ttl 0 --type-cache-ttl 0 --only-dir bert \
        --implicit-dirs dlvm-dataset /mnt/bert

batch_size=${1:-"4"}
learning_rate=${2:-"5e-6"}
precision=${3:-"fp16"}
use_xla=${4:-"true"}
num_gpu=${5:-"4"}
seq_length=${6:-"384"}
doc_stride=${7:-"128"}
bert_model=${8:-"large"}
squad_version=${9:-"1.1"}
epochs=${10:-"2.0"}

BASE_DIR=/mnt/GITHUB
BERT_DIR=$BASE_DIR/checkpoint/bert_tf_v1_1_large_fp16_384_v2
DATA_DIR=$BASE_DIR/squad
RESULT_DIR=$BASE_DIR/output
printf "Saving checkpoints to %s\n" "$RESULT_DIR"
export CUDA_VISIBLE_DEVICES=0,1,3,2,7,6,4,5

mpirun -np $num_gpu -H localhost:$num_gpu \
    --allow-run-as-root -bind-to none -map-by slot \
    python run_squad.py --vocab_file=$BERT_DIR/vocab.txt \
     --bert_config_file=$BERT_DIR/bert_config.json \
     --init_checkpoint=$BERT_DIR/model.ckpt-5474 \
     --output_dir=$RESULT_DIR --train_batch_size=$batch_size \
     --do_predict=True --predict_file=$DATA_DIR/dev-v1.1.json \
     --eval_script=$DATA_DIR/evaluate-v1.1.py \
     --do_train=True --train_file=$DATA_DIR/train-v1.1.json \
     --learning_rate=$learning_rate \
     --num_train_epochs=$epochs \
     --max_seq_length=$seq_length \
     --doc_stride=$doc_stride \
     --save_checkpoints_steps 1000 \
     --horovod --amp --use_xla

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building NLP Solutions with NGC Models and Containers on Google Cloud AI Platform**  \n",
    "## **Table of Contents**\n",
    "- [Repository](#repository)\n",
    "- [Task](#task)\n",
    "- [Assets](#assets)\n",
    "- [Data](#data)\n",
    "- [Setup](#setup)\n",
    "- [Fine-tune](#fine-tune)\n",
    "- [TensorRT Optimization](#tensorrt-optimization)\n",
    "- [Predict](#predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Repository**\n",
    "\n",
    "- [Demo Repository - https://github.com/sohn21c/gcp-ai-platform-ngc-examples](https://github.com/sohn21c/gcp-ai-platform-ngc-examples)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "1. Fine-tune BERT with SQuAD data set to create QA model\n",
    "2. Optimize the model with NVIDIA TensorRT\n",
    "3. Deploy the model with NVIDIA Triton Inference Server\n",
    "\n",
    "#### Relevant Resources\n",
    "1. [Cloud AI Platform](https://cloud.google.com/ai-platform)  \n",
    "    - [Training - https://cloud.google.com/ai-platform/training/docs/overview](https://cloud.google.com/ai-platform/training/docs/overview)\n",
    "    - [Prediction - https://cloud.google.com/ai-platform/prediction/docs](https://cloud.google.com/ai-platform/prediction/docs)\n",
    "    - [Notebook - https://cloud.google.com/ai-platform-notebooks](https://cloud.google.com/ai-platform-notebooks)\n",
    "2. [NGC](https://www.nvidia.com/en-us/gpu-cloud/)\n",
    "    - [NGC Catalog - https://ngc.nvidia.com/catalog/all](https://ngc.nvidia.com/catalog/all](https://ngc.nvidia.com/catalog/all](https://ngc.nvidia.com/catalog/all)\n",
    "<!-- <p>&nbsp;</p> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assets**\n",
    "#### BERT Resources\n",
    "- [https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow](https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow)  \n",
    "\n",
    "#### BERT Config / Vocab / Pretrained Checkpoint\n",
    "- [https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_pretraining_amp_lamb](https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_pretraining_amp_lamb)\n",
    "\n",
    "#### TensorRT\n",
    "- [https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt](https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt)\n",
    "\n",
    "#### Triton Inference Server \n",
    "- [https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver](https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data**\n",
    "#### Stanford Question Answering Dataset(SQuAD)\n",
    "- [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "\n",
    "- SQuAD is a reading comprehension dataset.\n",
    "    - **SQuAD 1.1**: 100,000+ question-answer pairs\n",
    "    - **SQuAD 2.0**: SQuAD 1.1 + 50,000+ unanswerable questions \n",
    "\n",
    "#### Data Format\n",
    "- JSON:\n",
    "    - **qas**: question, answer, answer_start (index)\n",
    "    - **context**\n",
    "\n",
    "#### Example Question-Answer Pair\n",
    "![squad](https://github.com/sohn21c/gcp-ai-platform-ngc-examples/blob/master/imgs/squad.png?raw=true)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "#### Download And Save Data to Google Cloud Storage\n",
    "- Web Browser\n",
    "- Cloud Shell\n",
    "    - Create GS bucket\n",
    "        ```bash\n",
    "        gsutil mb gs://gtc-demo\n",
    "        ```  \n",
    "    <p>&nbsp;</p>\n",
    "    - Enable GCSFuse with Cloud Shell\n",
    "        ```bash\n",
    "        # Replace [BUCKET_NAME], [USER] and [FOLDER_NAME] with yours \n",
    "        gsutil mb gs://[BUCKET_NAME]/\n",
    "        mkdir /home/[USER]/[FOLDER_NAME]\n",
    "        chmod 777 /home/[USER]/[FOLDER_NAME]\n",
    "        gcsfuse -o nonempty -file-mode=777 -dir-mode=777 --stat-cache-ttl 0 --type-cache-ttl 0 [BUCKET_NAME] /home/[USER\n",
    "        ]/[FOLDER_NAME]\n",
    "        ```  \n",
    "    <p>&nbsp;</p>\n",
    "    - Download SQuAD v1.1 dataset to GCS\n",
    "        ```bash\n",
    "        mkdir -p /home/[USER]/[FOLDER_NAME]/squad/v1.1\n",
    "        cd /home/[USER]/[FOLDER_NAME]/squad/v1.1\n",
    "        wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "        wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
    "        wget https://worksheets.codalab.org/rest/bundles/0xbcd57bee090b421c982906709c8c27e1/contents/blob/\n",
    "        ```  \n",
    "    <p>&nbsp;</p>\n",
    "    - Download checkpoint and BERT files\n",
    "        ```bash\n",
    "        mkdir -p /home/[USER]/[FOLDER_NAME]/checkpoint\n",
    "        cd /home/[USER]/[FOLDER_NAME]/checkpoint\n",
    "        wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/bert_tf_ckpt_large_pretraining_amp_lamb/versions/19.03.0/zip -O bert_tf_files.zip\n",
    "\n",
    "        unzip bert_tf_files.zip -d bert_tf_pretraining_lamb_16n_v1\n",
    "        ```\n",
    "    <p>&nbsp;</p>\n",
    "    - Create {output, trt_deployment, trt_engine} dir\n",
    "        ```bash\n",
    "        mkdir -p /home/[USER]/[FOLDER_NAME]/{output, trt_deployment, trt_engine}\n",
    "        ```\n",
    "\n",
    "#### Storage Structure\n",
    "\n",
    "```\n",
    "├── checkpoint  \n",
    "│     ├── bert_tf_pretraining_lamb_16n_v1      \n",
    "├── output \n",
    "├── squad  \n",
    "│     ├── v1.1  \n",
    "│           ├──dev-v1.1.json  \n",
    "│           ├──evaluate-v1.1.py \n",
    "│           ├──train-v1.1.json  \n",
    "├── trt_deployment  \n",
    "├── trt_engine  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine-tune**\n",
    "#### Work Flow\n",
    "1. Build the container in Cloud Shell\n",
    "2. Push container to Google Cloud Registry\n",
    "3. Submit Training Job (Fine-tune)\n",
    "\n",
    "#### Resources\n",
    "- [Fine-tuning](https://github.com/sohn21c/gcp-ai-platform-ngc-examples/tree/master/ngc_bert_finetuning)\n",
    "- [BERT for TF @NGC](https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow)\n",
    "- [Cloud AI Platform Training](https://cloud.google.com/ai-platform/training/docs/overview)\n",
    "\n",
    "#### Cloud SDK\n",
    "- [Cloud SDK Documentation](https://cloud.google.com/sdk/gcloud/reference/ai-platform)  \n",
    "\n",
    "#### Submit Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"FINE_TUNE_JOB_NAME\"] = \"bert_finetuning_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training $FINE_TUNE_JOB_NAME \\\n",
    "    --master-image-uri gcr.io/k80-exploration/gtc_demo_bert:latest \\\n",
    "    --region us-central1 \\\n",
    "    --master-accelerator count=8,type=nvidia-tesla-v100 \\\n",
    "    --master-machine-type n1-highmem-96 \\\n",
    "    --scale-tier custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs describe $FINE_TUNE_JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TensorRT Optmization**  \n",
    "#### [TensorRT](https://github.com/NVIDIA/TensorRT)\n",
    "![TensorRT](https://github.com/sohn21c/gcp-ai-platform-ngc-examples/blob/master/imgs/tensorrt.png?raw=true)  \n",
    "\n",
    "#### Work Flow\n",
    "1. Build the container in Cloud Shell\n",
    "2. Push container to Google Cloud Container Registry\n",
    "3. Submit Training Job (Optimization)\n",
    "\n",
    "#### Resources\n",
    "- [TensorRT Optimization](https://github.com/sohn21c/gcp-ai-platform-ngc-examples/tree/master/ngc_trt_optimization)\n",
    "- [TensorRT](https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt)\n",
    "\n",
    "#### Submit Optimization Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRT_JOB_NAME\"] = \"bert_trt_demo_00\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training $TRT_JOB_NAME \\\n",
    "    --master-image-uri gcr.io/k80-exploration/gtc_demo_trt_bert:latest \\\n",
    "    --region us-central1 \\\n",
    "    --master-accelerator count=1,type=nvidia-tesla-t4 \\\n",
    "    --master-machine-type n1-highmem-8 \\\n",
    "    --scale-tier custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs describe $TRT_JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move TRT Engine to Triton Model Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://gtc-bert-demo/bert/trt_engine/bert_large_384_int8.engine gs://gtc-bert-demo/bert/trt_deployment/bert/1/model.plan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA Triton Inference Server\n",
    "![Triton](https://github.com/sohn21c/gcp-ai-platform-ngc-examples/blob/master/imgs/triton.png?raw=true)  \n",
    "\n",
    "\n",
    "#### Work Flow\n",
    "1. Build the container in Cloud Shell\n",
    "2. Push Triton container to Google Cloud Container Registry\n",
    "3. Create Model\n",
    "4. Create Model Version\n",
    "5. Pre-process context/question for inference\n",
    "6. Submit Prediction Job\n",
    "7. Post-process response from Triton \n",
    "\n",
    "#### CMD\n",
    "```bash\n",
    "export CAIP_IMAGE=gcr.io/k80-exploration/tritonserver:20.08-py3\n",
    "docker pull nvcr.io/nvidia/tritonserver:20.08-py3\n",
    "docker tag nvcr.io/nvidia/tritonserver:20.08-py3 ${CAIP_IMAGE}\n",
    "docker push ${CAIP_IMAGE}\n",
    "```\n",
    "\n",
    "#### Resources\n",
    "- [Triton Deployment](https://github.com/sohn21c/gcp-ai-platform-ngc-examples/tree/master/ngc_triton_bert_deployment)\n",
    "- [Triton Inference Server](https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver)\n",
    "- [Prediction Overview](https://cloud.google.com/ai-platform/prediction/docs/overview)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROJECT_ID=k80-exploration\n",
    "%env MODEL_GCS_PATH=gs://gtc-bert-demo/bert/trt_deployment\n",
    "%env ENDPOINT=https://alpha-ml.googleapis.com/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model\n",
    "AI Platform Prediction uses a **Model / Model Version** Hierarchy, where the Model is a logical grouping of Model Versions.  We will first create the Model.  \n",
    "Because the MODEL_NAME variable will be used later to specify the predict route, and Triton will use that route to run prediction on a specific model, we must set the value of this variable to a valid name of a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X \\\n",
    "    POST -k -H \"Content-Type: application/json\" \\\n",
    "    -d \"{'name': '\"$MODEL_NAME\"'}\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gcloud equivalent*_  \n",
    "  \n",
    "```bash\n",
    "gcloud ai-platform models create $MODEL_NAME --region us-central1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Version\n",
    "After the Model is created, we can now create a Model Version under this Model.  Each Model Version will need a name that is unique within the Model.  In AI Platform Prediction Custom Container, a `{Project}/{Model}/{ModelVersion}` uniquely identifies the specific container and model artifact used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env VERSION_NAME=demo02\n",
    "%env TRITON_MODEL_NAME=bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "triton_bert_version = {\n",
    "  \"name\": os.getenv(\"VERSION_NAME\"),\n",
    "  \"deployment_uri\": os.getenv(\"MODEL_GCS_PATH\"),\n",
    "  \"container\": {\n",
    "    \"image\": \"gcr.io/\"+os.getenv(\"PROJECT_ID\")+\"/tritonserver:20.08-py3\",\n",
    "    \"args\": [\"tritonserver\",\n",
    "             \"--model-repository=$(AIP_STORAGE_URI)\",\n",
    "             \"--strict-model-config=false\"\n",
    "    ],\n",
    "    \"env\": [\n",
    "    ], \n",
    "    \"ports\": [\n",
    "      { \"containerPort\": 8000 }\n",
    "    ]\n",
    "  },\n",
    "  \"routes\": {\n",
    "    \"predict\": \"/v2/models/\"+os.getenv(\"TRITON_MODEL_NAME\")+\"/infer\",\n",
    "    \"health\": \"/v2/models/\"+os.getenv(\"TRITON_MODEL_NAME\")\n",
    "  },\n",
    "  \"machine_type\": \"n1-standard-4\",\n",
    "  \"acceleratorConfig\": {\n",
    "    \"count\":1,\n",
    "    \"type\":\"nvidia-tesla-t4\"\n",
    "  },\n",
    "  \"autoScaling\": {\n",
    "    \"minNodes\": 1\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"triton_bert_version.json\", \"w\") as f: \n",
    "  json.dump(triton_bert_version, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X \\\n",
    "    POST -k -H \"Content-Type: application/json\" \\\n",
    "    -d @triton_bert_version.json \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/${MODEL_NAME}/versions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gcloud equivalent*_  \n",
    "```python\n",
    "import yaml\n",
    "\n",
    "config_simple={'deploymentUri': os.getenv(\"MODEL_GCS_PATH\"), \\\n",
    "               'container': {'image': \"gcr.io/\"+os.getenv(\"PROJECT_ID\")+\"/tritonserver:20.08-py3\", \\\n",
    "                             'args': ['tritonserver', \n",
    "                                      '--model-repository=$(AIP_STORAGE_URI)',\n",
    "                                      '--strict-model-config=false'], \\\n",
    "                             'env': [], \\\n",
    "                             'ports': {'containerPort': 8000}}, \\\n",
    "               'routes': {'predict': '/v2/models/'+os.getenv(\"TRITON_MODEL_NAME\")+'/infer', \\\n",
    "                          'health': '/v2/models/'+os.getenv(\"TRITON_MODEL_NAME\")}, \\\n",
    "               'machineType': 'n1-standard-4', 'autoScaling': {'minNodes': 1}}\n",
    "\n",
    "with open(r'triton_bert_version.yaml', 'w') as file:\n",
    "    config = yaml.dump(config_simple, file)\n",
    "```    \n",
    "```bash\n",
    "gcloud ai-platform versions create $VERSION_NAME \\\n",
    "--model $MODEL_NAME \\\n",
    "--accelerator count=1,type=nvidia-tesla-t4 \\\n",
    "--config triton_bert_version.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the status of Model Version creation\n",
    "Creating a Model Version may take several minutes.  You can check on the status of this specfic Model Version with the following, and a successful deployment will show:  \n",
    "  \n",
    "`\"state\": \"READY\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET -k -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/${MODEL_NAME}/versions/${VERSION_NAME}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gcloud equivalent*_  \n",
    "```bash\n",
    "gcloud ai-platform versions describe $VERSION_NAME --model=$MODEL_NAME\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all Model Versions and their status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET -k -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/${MODEL_NAME}/versions/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "_gcloud equivalent*_  \n",
    "```bash\n",
    "gcloud ai-platform versions list --model=$MODEL_NAME\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_squad import get_predictions, RawResult\n",
    "from urllib.parse import quote\n",
    "from utils.create_squad_data import read_squad_examples, convert_examples_to_features\n",
    "from utils.caip_predict import *\n",
    "\n",
    "import json\n",
    "import modeling\n",
    "import numpy as np\n",
    "import os  \n",
    "import tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "title = \"Project Apollo\"\n",
    "\n",
    "context = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of landing a man on the Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\"\n",
    "\n",
    "question = \"What project put the first Americans into space?\"\n",
    "\n",
    "body = {\n",
    "    \"data\": [\n",
    "        {\"title\": title, \n",
    "         \"paragraphs\": [\n",
    "             {\"context\": context, \n",
    "              \"qas\": [\n",
    "                  { \"question\": question, \n",
    "                   \"id\": \"Q1\"}\n",
    "              ]}\n",
    "         ]}\n",
    "    ]}\n",
    "\n",
    "vocab_file = 'vocab.txt'\n",
    "\n",
    "print(json.dumps(body, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"TensorRT\"\n",
    "\n",
    "context = \"TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\"\n",
    "\n",
    "question = \"What is TensorRT?\"\n",
    "\n",
    "body = {\n",
    "    \"data\": [\n",
    "        {\"title\": title, \n",
    "         \"paragraphs\": [\n",
    "             {\"context\": context, \n",
    "              \"qas\": [\n",
    "                  { \"question\": question, \n",
    "                   \"id\": \"Q1\"}\n",
    "              ]}\n",
    "         ]}\n",
    "    ]}\n",
    "\n",
    "vocab_file = 'vocab.txt'\n",
    "\n",
    "print(json.dumps(body, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bert_config():\n",
    "    \"\"\"\n",
    "    Defines the configuration of BERT model\n",
    "    \"\"\"\n",
    "    global do_lower_case \n",
    "    global predict_batch_size\n",
    "    global max_seq_length\n",
    "    global doc_stride \n",
    "    global max_query_length \n",
    "    global verbose_logging \n",
    "    global version_2_with_negative \n",
    "    global n_best_size\n",
    "    global max_answer_length\n",
    "\n",
    "    # Set True for uncased model\n",
    "    do_lower_case = True\n",
    "\n",
    "    # Total batch size for predictions\n",
    "    predict_batch_size = 1\n",
    "\n",
    "    # The maximum total input sequence length after WordPiece tokenization. \n",
    "    # Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "    max_seq_length = 384\n",
    "\n",
    "    # When splitting up a long document into chunks, how much stride to take between chunks.\n",
    "    doc_stride = 128\n",
    "\n",
    "    # The maximum number of tokens for the question. \n",
    "    # Questions longer than this will be truncated to this length.\n",
    "    max_query_length = 64\n",
    "\n",
    "    # Set True for verbosity\n",
    "    verbose_logging = True\n",
    "\n",
    "    # Set True if the dataset has samples with no answers. For SQuAD 1.1, this is set to False\n",
    "    version_2_with_negative = False\n",
    "\n",
    "    # The total number of n-best predictions to generate in the nbest_predictions.json output file.\n",
    "    n_best_size = 20\n",
    "\n",
    "    # The maximum length of an answer that can be generated. \n",
    "    # This is needed  because the start and end predictions are not conditioned on one another.\n",
    "    max_answer_length = 30\n",
    "\n",
    "    return\n",
    "\n",
    "init_bert_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "input_data = body['data']\n",
    "\n",
    "eval_examples = read_squad_examples(input_file=None, is_training=False, version_2_with_negative=version_2_with_negative, input_data=input_data)\n",
    "eval_features = []\n",
    "def append_feature(feature):\n",
    "    eval_features.append(feature)    \n",
    "convert_examples_to_features(\n",
    "    examples=eval_examples[0:],\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    max_query_length=max_query_length,\n",
    "    is_training=False,\n",
    "    output_fn=append_feature)\n",
    "\n",
    "inputs_dict = batch(eval_features)\n",
    "label_ids_data = np.stack(inputs_dict['unique_ids'])\n",
    "input_ids_data = np.stack(inputs_dict['input_ids'])\n",
    "input_mask_data = np.stack(inputs_dict['input_mask'])\n",
    "segment_ids_data = np.stack(inputs_dict['segment_ids'])\n",
    "\n",
    "inputs = []\n",
    "inputs.append(InferInput('input_ids', input_ids_data.shape, \"INT32\"))\n",
    "inputs[0].set_data_from_numpy(input_ids_data, binary_data=False)\n",
    "inputs.append(InferInput('input_mask', input_mask_data.shape, \"INT32\"))\n",
    "inputs[1].set_data_from_numpy(input_mask_data, binary_data=False)\n",
    "inputs.append(InferInput('segment_ids', segment_ids_data.shape, \"INT32\"))\n",
    "inputs[2].set_data_from_numpy(segment_ids_data, binary_data=False)\n",
    "\n",
    "outputs = []\n",
    "outputs.append(InferRequestedOutput('cls_squad_logits', binary_data=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Prediction Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trt_inference_request(inputs):\n",
    "    infer_request = {\"inputs\":[]}\n",
    "    temp = {\n",
    "        \"name\": \"\",\n",
    "        \"shape\": None,\n",
    "        \"datatype\": \"\",\n",
    "        \"parameters\": {},\n",
    "        \"data\": None\n",
    "    }\n",
    "    for inp in inputs:\n",
    "        arg_obj = temp.copy()\n",
    "        arg_obj[\"name\"] = inp._name\n",
    "        arg_obj[\"shape\"] = list(inp._shape)\n",
    "        arg_obj[\"datatype\"] = inp._datatype\n",
    "        arg_obj[\"data\"] = inp._data\n",
    "        infer_request[\"inputs\"].append(arg_obj)\n",
    "    return infer_request\n",
    "\n",
    "request_body = get_trt_inference_request(inputs)\n",
    "\n",
    "with open('payload.dat', 'w') as output_file:\n",
    "    json.dump(request_body, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env VERSION_NAME=demo01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST $ENDPOINT/projects/$PROJECT_ID/models/$MODEL_NAME/versions/$VERSION_NAME:predict \\\n",
    "    -k -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    -d @payload.dat \\\n",
    "    -o response.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response.txt', 'r') as response_file:\n",
    "    response = response_file.read()\n",
    "response_body = json.loads(response)\n",
    "response_data = response_body['outputs'][0]['data']\n",
    "print(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits_results, end_logits_results = [], []\n",
    "for ind, val in enumerate(response_data):\n",
    "    if ind % 2 == 0:\n",
    "        start_logits_results.append(val)\n",
    "    else:\n",
    "        end_logits_results.append(val)\n",
    "        \n",
    "all_results = []\n",
    "unique_id = inputs_dict['unique_ids'][0][0]\n",
    "start_logits = [float(x) for x in start_logits_results]\n",
    "end_logits = [float(x) for x in end_logits_results]\n",
    "all_results.append(\n",
    "    RawResult(\n",
    "        unique_id=unique_id,\n",
    "        start_logits=start_logits,\n",
    "        end_logits=end_logits)\n",
    ")\n",
    "all_predictions, all_nbest_json, scores_diff_json = get_predictions(\n",
    "                  eval_examples, eval_features, all_results,\n",
    "                  n_best_size, max_answer_length,\n",
    "                  do_lower_case, version_2_with_negative,\n",
    "                  verbose_logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Context: ')\n",
    "print(f'\\t{context}')\n",
    "print()\n",
    "print('Question: ')\n",
    "print(f'\\t{question}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction: ')\n",
    "print('\\t',all_predictions['Q1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove model version\n",
    "!curl --request DELETE -k -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/${MODEL_NAME}/versions/${VERSION_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gcloud equivalent*_  \n",
    "```bash\n",
    "gcloud ai-platform versions delete $VERSION_NAME --model=$MODEL_NAME --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove model \n",
    "!curl --request DELETE -k -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/${MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gcloud equivalent*_  \n",
    "```bash\n",
    "gcloud ai-platform models delete $MODEL_NAME --quiet\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

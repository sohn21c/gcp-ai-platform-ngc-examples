### AI Platform TF BERT Deployment with NVIDIA Triton Inference Server on GPU Guide

* Task summary:
    Deploy TensorRT optimized BERT QA model with Triton Inference Server on Google Cloud AI Platform, and predict with CAIP Predict service.  

* User has to follow `bert_on_caip.ipynb`, `Prediction` section specifically, to deploy optimized BERT model on Google Cloud AI Platorm Prediction for inference.  

* For more information, please refer to **GTC 2020 Building NLP Solutions with NGC Models and Containers on Google Cloud AI Platform [A21324]**